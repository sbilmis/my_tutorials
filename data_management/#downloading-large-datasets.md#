# Tutorial: How to Download Massive Datasets from Zenodo

!!! abstract "Overview"
    * **Goal:** Download the 330GB OpenAIRE Graph dataset safely.
    * **Time Required:** ~10 minutes to set up (download time depends on bandwidth).
    * **Skill Level:** Beginner / Intermediate.
    * **Prerequisites:** Access to a terminal (Linux/macOS) and ~350GB of free disk space.
    * **Tools Used:** `zenodo_get` (for link generation), `aria2c` (recommended for download).

This guide explains how to reliably download massive datasets (100GB to terabytes) from Zenodo to a local server or a High-Performance Computing (HPC) cluster.

As a practical example, we will be using the **OpenAIRE Graph** dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large text corpora).

---

## 1. Context: What are we downloading?

### What is Zenodo?
Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from *any* field of research and issues a persistent DOI for every record.

### The Example: OpenAIRE Graph
In this tutorial, we are downloading the **OpenAIRE Graph**, one of the worldâ€™s largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.

* **Note on freshness:** Massive datasets on Zenodo are usually **static snapshots**. For example, the OpenAIRE Graph dump is published roughly every six months. While live portals show real-time data, the Zenodo dump is the standard choice for stable, offline analysis.

---

## 2. The â€œGolden Ruleâ€ of Large Downloads

!!! danger "Do NOT use the â€œDownload allâ€ button"
    Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process
    will time out, does not support resuming, and provides no checksum verification.
    **Always download files individually.**

On Zenodo record pages you may see a â€œDownload allâ€ button pointing to a `files-archive` link.

* **Why avoid it?** Zenodo tries to create a single huge zip stream on the fly.
* **Consequence:** If the download fails near the end, you must restart from zero.

**Solution:** Always download files individually using scripted tools.

---

## 3. Method 1: The Easiest Way (`zenodo_get`)

`zenodo_get` is a community-maintained Python tool that handles file lists, retries, and checksum verification automatically.

!!! warning "Limitation: No Parallel Downloads"
    `zenodo_get` downloads files **sequentially** (one by one). It cannot be parallelized to download multiple files at the same time. If you have many large files and high bandwidth, this method will be significantly slower than Method 2 (`aria2c`).

### Step A: Installation

```bash
pip install zenodo-get
```

### Step B: Identify the Record ID

You only need the **record ID** from the dataset URL.

* Example URL: `https://zenodo.org/records/17725827`
* Record ID: `17725827`

### Step C: Download Command

```bash
zenodo_get 17725827 -R 5 -p 2
```

**Flag explanation:**

* `-R 5`: Retry failed downloads up to 5 times.
* `-p 2`: Pause 2 seconds between retries.

### Resuming a Failed Download

If the process stops, simply rerun the **same command**.

Completed and checksum-verified files will be skipped automatically.

---

## 4. Method 2: The Recommended High-Speed Way (`aria2c`)

For massive datasets, `aria2c` is superior because it supports **parallel operations** (downloading multiple files at once) and handles unstable connections robustly.

### Why use `aria2c`?

* **Parallelization:** Unlike `zenodo_get`, `aria2c` can download 16+ files simultaneously.
* **Connection Splitting:** It opens multiple connections per file to maximize bandwidth.
* **Resumability:** Excellent support for resuming interrupted downloads.

### Step A: Generate the URL List

We still use `zenodo_get` to fetch the download links, but we save them to a file instead of downloading the data.

```bash
zenodo_get 17725827 -w urls.txt

```

### Step B: Parallel Download with Browser Spoofing

Zenodo often blocks automated download managers with `403 Forbidden` errors. To bypass this, we must "trick" the server into thinking `aria2c` is a standard web browser using the User-Agent flag.

Run the following command:

```bash
aria2c -c -i urls.txt -j 16 -x 16 \
-U "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

```

**Flag explanation:**

* `-c`: **Continue** (Resume) interrupted downloads. Essential for large files.
* `-i urls.txt`: Input file containing the list of URLs.
* `-j 16`: **Parallel Downloads.** Download 16 files simultaneously (saves massive time).
* `-x 16`: **Max Connections.** Use 16 connections per single file.
* `-U "..."`: **User-Agent.** Spoofs a Chrome browser user agent to prevent Zenodo from blocking the request as a "bot."

---

## 5. Method 3: The Standard Way (`wget`)

If you cannot install Python or `aria2c`, `wget` is usually available by default. Like `zenodo_get`, this is sequential (one by one).

### Download Using `wget`

```bash
wget -c -i urls.txt
```

* `-c`: Resume partially downloaded files.
* `-i`: Read URLs from a file.

---

## 6. Handling the Data: Read vs. Extract

After downloading, you will have several large `.tar` files.

### Option A: Recommended (Do NOT Extract)

!!! danger "STOP: Do NOT untar everything"
* **Risk:** 330GB of archives expands to **>9TB** when extracted.
* **Result:** You may exceed quotas or crash the filesystem.
* **Best practice:** Stream data directly from `.tar` files.

Many analysis scripts can read compressed archives directly, avoiding massive disk usage.

**Benefits:**

* Minimal disk usage
* Faster I/O
* No millions of tiny files

### Option B: If You MUST Extract (Advanced)

Only proceed if you have **>10TB free space** and a strict requirement to extract files.

**Safe extraction script:**

```bash
#!/bin/bash
for tarfile in *.tar; do
    dirname="${tarfile%.tar}"
    mkdir -p "$dirname"
    tar -xf "$tarfile" -C "$dirname"
done
```

---

## 7. Expert Mode: Getting Links Without `zenodo_get`

If Python is unavailable, you can query the Zenodo API directly.

### Generate `urls.txt` via `curl`

```bash
curl -s [https://zenodo.org/api/records/17725827](https://zenodo.org/api/records/17725827) \
| grep -oP '[https://zenodo.org/api/records/17725827/files/](https://zenodo.org/api/records/17725827/files/)[^"]+' \
> urls.txt

```

You can then use `wget` or `aria2c` with this file.

---

## ðŸ§  Knowledge Check

### Challenge 1: Generate Links Without Downloading

??? question "Solution"
`bash zenodo_get 17725827 -w links.txt grep "communities_infrastructures.tar" links.txt `

### Challenge 2: Download Only One Small File

??? question "Solution"
```bash
# wget
wget -c <URL>
```

# aria2c (remember to use the User-Agent flag if blocked!)
aria2c -x 10 -s 10 -U "Mozilla/5.0..." <URL>
```

```

### Challenge 3: Safe Extraction Test

??? success "Best practice"
`bash mkdir test_extract tar -xf communities_infrastructures.tar -C test_extract ls -l test_extract `

If files appear inside `test_extract`, you are ready for the full dataset.

```

```





# Tutorial: How to Download Massive Datasets from Zenodo

!!! abstract "Overview"
    * **Goal:** Download the 330GB OpenAIRE Graph dataset safely.
    * **Time Required:** ~10 minutes to set up (download time depends on bandwidth).
    * **Skill Level:** Beginner / Intermediate.
    * **Prerequisites:** Access to a terminal (Linux/macOS) and ~350GB of free disk space.
    * **Tools Used:** `zenodo_get`, `aria2c`, or `wget`.

This guide explains how to reliably download massive datasets (100GB to terabytes) from Zenodo to a local server or a High-Performance Computing (HPC) cluster.

As a practical example, we will be using the **OpenAIRE Graph** dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large text corpora).

---

## 1. Context: What are we downloading?

### What is Zenodo?
Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from *any* field of research and issues a persistent DOI for every record.

### The Example: OpenAIRE Graph
In this tutorial, we are downloading the **OpenAIRE Graph**, one of the worldâ€™s largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.

* **Note on freshness:** Massive datasets on Zenodo are usually **static snapshots**. For example, the OpenAIRE Graph dump is published roughly every six months. While live portals show real-time data, the Zenodo dump is the standard choice for stable, offline analysis.

---

## 2. The â€œGolden Ruleâ€ of Large Downloads

!!! danger "Do NOT use the â€œDownload allâ€ button"
    Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process
    will time out, does not support resuming, and provides no checksum verification.
    **Always download files individually.**

On Zenodo record pages you may see a â€œDownload allâ€ button pointing to a `files-archive` link.

* **Why avoid it?** Zenodo tries to create a single huge zip stream on the fly.
* **Consequence:** If the download fails near the end, you must restart from zero.

**Solution:** Always download files individually using scripted tools.

---

## 3. Method 1: The Easiest Way (`zenodo_get`)

`zenodo_get` is a community-maintained Python tool that handles file lists, retries, and checksum verification automatically.

### Step A: Installation

```bash
pip install zenodo-get
```

### Step B: Identify the Record ID

You only need the **record ID** from the dataset URL.

* Example URL: `https://zenodo.org/records/17725827`
* Record ID: `17725827`

### Step C: Robust Download Command

```bash
zenodo_get 17725827 -R 5 -p 2
```

**Flag explanation:**

* `-R 5`: Retry failed downloads up to 5 times.
* `-p 2`: Pause 2 seconds between retries.

### Resuming a Failed Download

If the process stops, simply rerun the **same command**.  
Completed and checksum-verified files will be skipped automatically.

---

## 4. Method 2: The High-Speed Way (`aria2c`)

`aria2c` is ideal for fast but unstable connections. It supports parallel connections per file.

### Advantages

* Parallel downloads per file
* Excellent resume support
* Efficient bandwidth utilization

### Step A: Generate the URL List

```bash
zenodo_get 17725827 -w urls.txt
```

### Step B: Download with `aria2c`

```bash
aria2c -i urls.txt -x 10 -c
```

* `-x 10`: Up to 10 connections per file
* `-c`: Resume interrupted downloads

---

## 5. Method 3: The Standard Way (`wget`)

If you cannot install Python or `aria2c`, `wget` is usually available by default.

### Download Using `wget`

```bash
wget -c -i urls.txt
```

* `-c`: Resume partially downloaded files
* `-i`: Read URLs from a file

---

## 6. Handling the Data: Read vs. Extract

After downloading, you will have several large `.tar` files.

### Option A: Recommended (Do NOT Extract)

!!! danger "STOP: Do NOT untar everything"
    * **Risk:** 330GB of archives expands to **>9TB** when extracted.
    * **Result:** You may exceed quotas or crash the filesystem.
    * **Best practice:** Stream data directly from `.tar` files.

Many analysis scripts can read compressed archives directly, avoiding massive disk usage.

**Benefits:**

* Minimal disk usage
* Faster I/O
* No millions of tiny files

### Option B: If You MUST Extract (Advanced)

Only proceed if you have **>10TB free space** and a strict requirement to extract files.

**Safe extraction script:**

```bash
#!/bin/bash
for tarfile in *.tar; do
    dirname="${tarfile%.tar}"
    mkdir -p "$dirname"
    tar -xf "$tarfile" -C "$dirname"
done
```

---

## 7. Expert Mode: Getting Links Without `zenodo_get`

If Python is unavailable, you can query the Zenodo API directly.

### Generate `urls.txt` via `curl`

```bash
curl -s https://zenodo.org/api/records/17725827 \
| grep -oP 'https://zenodo.org/api/records/17725827/files/[^"]+' \
> urls.txt
```

You can then use `wget` or `aria2c` with this file.

---

## ðŸ§  Knowledge Check

### Challenge 1: Generate Links Without Downloading

??? question "Solution"
```bash
zenodo_get 17725827 -w links.txt
grep "communities_infrastructures.tar" links.txt
```

### Challenge 2: Download Only One Small File

??? question "Solution"
```bash
# wget
wget -c <URL>

# aria2c
aria2c -x 10 -s 10 <URL>
```

### Challenge 3: Safe Extraction Test

??? success "Best practice"
```bash
mkdir test_extract
tar -xf communities_infrastructures.tar -C test_extract
ls -l test_extract
```

If files appear inside `test_extract`, you are ready for the full dataset.
