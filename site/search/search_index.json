{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to My Digital Notebook","text":"<p>This site is a small personal notebook where I write down technical notes and short tutorials that I use in my own work.</p> <p>The main purpose is simple: to avoid re-learning the same things again months later.</p> <p>Some of these notes may also be useful to others working with Linux systems, data handling, or research workflows.</p> <p>This notebook grows slowly and irregularly, depending on real needs.</p>"},{"location":"data_management/downloading-large-datasets/","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":"<p>Overview</p> <ul> <li>Goal: Download the 330GB OpenAIRE Graph dataset safely.</li> <li>Time Required: ~10 minutes to set up (download time depends on bandwidth).</li> <li>Skill Level: Beginner / Intermediate.</li> <li>Prerequisites: Access to a terminal (Linux/macOS) and ~350GB of free disk space.</li> <li>Tools Used: <code>zenodo_get</code> (for link generation), <code>aria2c</code> (recommended), or standard <code>xargs</code>.</li> </ul> <p>This guide explains how to reliably download massive datasets (100GB to terabytes) from Zenodo to a local server or a High-Performance Computing (HPC) cluster.</p> <p>As a practical example, we will be using the OpenAIRE Graph dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large text corpora).</p>"},{"location":"data_management/downloading-large-datasets/#1-context-what-are-we-downloading","title":"1. Context: What are we downloading?","text":""},{"location":"data_management/downloading-large-datasets/#what-is-zenodo","title":"What is Zenodo?","text":"<p>Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from any field of research and issues a persistent DOI for every record.</p>"},{"location":"data_management/downloading-large-datasets/#the-example-openaire-graph","title":"The Example: OpenAIRE Graph","text":"<p>In this tutorial, we are downloading the OpenAIRE Graph, one of the world\u2019s largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.</p> <ul> <li>Note on freshness: Massive datasets on Zenodo are usually static snapshots. For example, the OpenAIRE Graph dump is published roughly every six months. While live portals show real-time data, the Zenodo dump is the standard choice for stable, offline analysis.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#2-the-golden-rule-of-large-downloads","title":"2. The \u201cGolden Rule\u201d of Large Downloads","text":"<p>Do NOT use the \u201cDownload all\u201d button</p> <p>Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process will time out, does not support resuming, and provides no checksum verification. Always download files individually.</p> <p>On Zenodo record pages you may see a \u201cDownload all\u201d button pointing to a <code>files-archive</code> link.</p> <ul> <li>Why avoid it? Zenodo tries to create a single huge zip stream on the fly.</li> <li>Consequence: If the download fails near the end, you must restart from zero.</li> </ul> <p>Solution: Always download files individually using scripted tools.</p>"},{"location":"data_management/downloading-large-datasets/#3-method-1-the-easiest-way-zenodo_get","title":"3. Method 1: The Easiest Way (<code>zenodo_get</code>)","text":"<p><code>zenodo_get</code> is a community-maintained Python tool that handles file lists, retries, and checksum verification automatically.</p> <p>Limitation: No Parallel Downloads</p> <p><code>zenodo_get</code> downloads files sequentially (one by one). It cannot be parallelized to download multiple files at the same time. If you have many large files and high bandwidth, this method will be significantly slower than Method 2 (<code>aria2c</code>).</p>"},{"location":"data_management/downloading-large-datasets/#step-a-installation","title":"Step A: Installation","text":"<pre><code>pip install zenodo-get\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-identify-the-record-id","title":"Step B: Identify the Record ID","text":"<p>You only need the record ID from the dataset URL.</p> <ul> <li>Example URL: <code>https://zenodo.org/records/17725827</code></li> <li>Record ID: <code>17725827</code></li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-c-download-command","title":"Step C: Download Command","text":"<pre><code>zenodo_get 17725827 -R 5 -p 2\n</code></pre> <p>Flag explanation:</p> <ul> <li><code>-R 5</code>: Retry failed downloads up to 5 times.</li> <li><code>-p 2</code>: Pause 2 seconds between retries.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#4-method-2-the-recommended-high-speed-way-aria2c","title":"4. Method 2: The Recommended High-Speed Way (<code>aria2c</code>)","text":"<p>For massive datasets, <code>aria2c</code> is superior because it supports parallel operations (downloading multiple files at once) and handles unstable connections robustly.</p>"},{"location":"data_management/downloading-large-datasets/#why-use-aria2c","title":"Why use <code>aria2c</code>?","text":"<ul> <li>Parallelization: Unlike <code>zenodo_get</code>, <code>aria2c</code> can download 16+ files simultaneously.</li> <li>Connection Splitting: It opens multiple connections per file to maximize bandwidth.</li> <li>Resumability: Excellent support for resuming interrupted downloads.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-a-generate-the-url-list","title":"Step A: Generate the URL List","text":"<p>We still use <code>zenodo_get</code> to fetch the download links, but we save them to a file instead of downloading the data.</p> <pre><code>zenodo_get 17725827 -w urls.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-parallel-download-with-browser-spoofing","title":"Step B: Parallel Download (With Browser Spoofing)","text":"<p>Zenodo frequently blocks automated download managers with <code>403 Forbidden</code> errors. To avoid this, we must trick the server into thinking we are a standard web browser by setting the User-Agent.</p> <p>Run the following command:</p> <pre><code>aria2c -c -i urls.txt -j 16 -x 16 \\\n-U \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n</code></pre> <p>Flag explanation:</p> <ul> <li><code>-c</code>: Continue (Resume). This is critical. If the download stops, this flag ensures it picks up exactly where it left off.</li> <li><code>-i urls.txt</code>: Input file containing the list of URLs.</li> <li><code>-j 16</code>: Parallel Downloads. Download 16 files simultaneously.</li> <li><code>-x 16</code>: Max Connections. Use 16 connections per single file.</li> <li><code>-U \"...\"</code>: User-Agent. Spoofs a Chrome browser to prevent 403 errors.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#5-method-3-the-sysadmin-way-gnu-parallel-xargs","title":"5. Method 3: The \"Sysadmin\" Way (GNU Parallel / xargs)","text":"<p>If you are on a restricted server where you cannot install <code>aria2c</code> or Python packages, you can use standard Linux tools (<code>wget</code> and <code>xargs</code>) to achieve parallel downloads.</p>"},{"location":"data_management/downloading-large-datasets/#the-command","title":"The Command","text":"<p>This command reads the URL list and spawns 8 separate <code>wget</code> processes at once.</p> <pre><code>cat urls.txt | xargs -n 1 -P 8 wget -q -c\n</code></pre> <p>Flag explanation:</p> <ul> <li><code>xargs</code>: A tool to build and execute command lines from standard input.</li> <li><code>-n 1</code>: Use 1 URL per command.</li> <li><code>-P 8</code>: Parallelism. Run up to 8 processes at the same time.</li> <li><code>wget -c</code>: The standard download tool with the continue flag enabled.</li> </ul> <p>Performance Note</p> <p>This method is heavier on system resources (CPU/RAM) than <code>aria2c</code> because it launches 8 full instances of <code>wget</code>. Use it only if <code>aria2c</code> is unavailable.</p>"},{"location":"data_management/downloading-large-datasets/#6-handling-the-data-read-vs-extract","title":"6. Handling the Data: Read vs. Extract","text":"<p>After downloading, you will have several large <code>.tar</code> files.</p>"},{"location":"data_management/downloading-large-datasets/#option-a-recommended-do-not-extract","title":"Option A: Recommended (Do NOT Extract)","text":"<p>STOP: Do NOT untar everything</p> <ul> <li>Risk: 330GB of archives expands to &gt;9TB when extracted.</li> <li>Result: You may exceed quotas or crash the filesystem.</li> <li>Best practice: Stream data directly from <code>.tar</code> files.</li> </ul> <p>Many analysis scripts can read compressed archives directly, avoiding massive disk usage.</p>"},{"location":"data_management/downloading-large-datasets/#option-b-if-you-must-extract-advanced","title":"Option B: If You MUST Extract (Advanced)","text":"<p>Only proceed if you have &gt;10TB free space and a strict requirement to extract files.</p> <p>Safe extraction script:</p> <pre><code>#!/bin/bash\nfor tarfile in *.tar; do\n    dirname=\"${tarfile%.tar}\"\n    mkdir -p \"$dirname\"\n    tar -xf \"$tarfile\" -C \"$dirname\"\ndone\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#7-expert-mode-getting-links-without-zenodo_get","title":"7. Expert Mode: Getting Links Without <code>zenodo_get</code>","text":"<p>If Python is unavailable, you can query the Zenodo API directly to generate your <code>urls.txt</code>.</p> <pre><code>curl -s [https://zenodo.org/api/records/17725827](https://zenodo.org/api/records/17725827) \\\n| grep -oP '[https://zenodo.org/api/records/17725827/files/](https://zenodo.org/api/records/17725827/files/)[^\"]+' \\\n&gt; urls.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#knowledge-check","title":"\ud83e\udde0 Knowledge Check","text":""},{"location":"data_management/downloading-large-datasets/#challenge-1-generate-links-without-downloading","title":"Challenge 1: Generate Links Without Downloading","text":"Solution <pre><code>zenodo_get 17725827 -w links.txt\ngrep \"communities_infrastructures.tar\" links.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-2-download-only-one-small-file","title":"Challenge 2: Download Only One Small File","text":"Solution <pre><code># Option 1: wget\nwget -c &lt;URL&gt;\n</code></pre> <p>Option 2: aria2c (Use User-Agent if blocked!) <pre><code>aria2c -x 10 -s 10 -U \"Mozilla/5.0...\" &lt;URL&gt;\n</code></pre></p>"},{"location":"data_management/downloading-large-datasets/#challenge-3-safe-extraction-test","title":"Challenge 3: Safe Extraction Test","text":"Best practice <pre><code>mkdir test_extract\ntar -xf communities_infrastructures.tar -C test_extract\nls -l test_extract\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-4-why-did-my-download-fail-with-403","title":"Challenge 4: Why did my download fail with 403?","text":"Answer <p>Zenodo likely detected your script as a bot. You must add a User-Agent string (<code>-U \"Mozilla/...\"</code>) to your <code>aria2c</code> command to pretend you are a browser.</p>"},{"location":"emacs/","title":"Emacs","text":""},{"location":"emacs/#org-mode","title":"ORG-Mode","text":""},{"location":"emacs/e1/","title":"Emacs","text":"<ul> <li>Coming soon.</li> </ul>"}]}