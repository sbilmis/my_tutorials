{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to My Digital Notebook","text":""},{"location":"#welcome-to-my-digital-notebook","title":"\ud83d\udc4b Welcome to My Digital Notebook","text":"<p>This site is a small personal notebook where I write down technical notes and short tutorials that I use in my own work.</p> <p>The main purpose is simple: to avoid re-learning the same things again months later.</p> <p>Some of these notes may also be useful to others working with Linux systems, data handling, or research workflows.</p> <p>This notebook grows slowly and irregularly, depending on real needs.</p>"},{"location":"data_management/downloading-large-datasets/","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":""},{"location":"data_management/downloading-large-datasets/#tutorial-how-to-download-massive-datasets-from-zenodo","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":"<p>Overview</p> <ul> <li>Goal: Download the 330GB OpenAIRE Graph dataset safely.</li> <li>Time Required: ~10 minutes to set up (download time depends on bandwidth).</li> <li>Skill Level: Beginner / Intermediate.</li> <li>Prerequisites: Access to a terminal (Linux/macOS) and ~350GB of free disk space.</li> <li>Tools Used: <code>zenodo_get</code>, <code>aria2c</code>, or <code>wget</code>.</li> </ul> <p>This guide explains how to reliably download massive datasets (100GB to terabytes) from Zenodo to a local server or a High-Performance Computing (HPC) cluster.</p> <p>As a practical example, we will be using the OpenAIRE Graph dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large text corpora).</p>"},{"location":"data_management/downloading-large-datasets/#1-context-what-are-we-downloading","title":"1. Context: What are we downloading?","text":""},{"location":"data_management/downloading-large-datasets/#what-is-zenodo","title":"What is Zenodo?","text":"<p>Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from any field of research and issues a persistent DOI for every record.</p>"},{"location":"data_management/downloading-large-datasets/#the-example-openaire-graph","title":"The Example: OpenAIRE Graph","text":"<p>In this tutorial, we are downloading the OpenAIRE Graph, one of the world\u2019s largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.</p> <ul> <li>Note on freshness: Massive datasets on Zenodo are usually static snapshots. For example, the OpenAIRE Graph dump is published roughly every six months. While live portals show real-time data, the Zenodo dump is the standard choice for stable, offline analysis.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#2-the-golden-rule-of-large-downloads","title":"2. The \u201cGolden Rule\u201d of Large Downloads","text":"<p>Do NOT use the \u201cDownload all\u201d button</p> <p>Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process will time out, does not support resuming, and provides no checksum verification. Always download files individually.</p> <p>On Zenodo record pages you may see a \u201cDownload all\u201d button pointing to a <code>files-archive</code> link.</p> <ul> <li>Why avoid it? Zenodo tries to create a single huge zip stream on the fly.</li> <li>Consequence: If the download fails near the end, you must restart from zero.</li> </ul> <p>Solution: Always download files individually using scripted tools.</p>"},{"location":"data_management/downloading-large-datasets/#3-method-1-the-easiest-way-zenodo_get","title":"3. Method 1: The Easiest Way (<code>zenodo_get</code>)","text":"<p><code>zenodo_get</code> is a community-maintained Python tool that handles file lists, retries, and checksum verification automatically.</p>"},{"location":"data_management/downloading-large-datasets/#step-a-installation","title":"Step A: Installation","text":"<pre><code>pip install zenodo-get\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-identify-the-record-id","title":"Step B: Identify the Record ID","text":"<p>You only need the record ID from the dataset URL.</p> <ul> <li>Example URL: <code>https://zenodo.org/records/17725827</code></li> <li>Record ID: <code>17725827</code></li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-c-robust-download-command","title":"Step C: Robust Download Command","text":"<pre><code>zenodo_get 17725827 -R 5 -p 2\n</code></pre> <p>Flag explanation:</p> <ul> <li><code>-R 5</code>: Retry failed downloads up to 5 times.</li> <li><code>-p 2</code>: Pause 2 seconds between retries.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#resuming-a-failed-download","title":"Resuming a Failed Download","text":"<p>If the process stops, simply rerun the same command. Completed and checksum-verified files will be skipped automatically.</p>"},{"location":"data_management/downloading-large-datasets/#4-method-2-the-high-speed-way-aria2c","title":"4. Method 2: The High-Speed Way (<code>aria2c</code>)","text":"<p><code>aria2c</code> is ideal for fast but unstable connections. It supports parallel connections per file.</p>"},{"location":"data_management/downloading-large-datasets/#advantages","title":"Advantages","text":"<ul> <li>Parallel downloads per file</li> <li>Excellent resume support</li> <li>Efficient bandwidth utilization</li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-a-generate-the-url-list","title":"Step A: Generate the URL List","text":"<pre><code>zenodo_get 17725827 -w urls.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-download-with-aria2c","title":"Step B: Download with <code>aria2c</code>","text":"<pre><code>aria2c -i urls.txt -x 10 -c\n</code></pre> <ul> <li><code>-x 10</code>: Up to 10 connections per file</li> <li><code>-c</code>: Resume interrupted downloads</li> </ul>"},{"location":"data_management/downloading-large-datasets/#5-method-3-the-standard-way-wget","title":"5. Method 3: The Standard Way (<code>wget</code>)","text":"<p>If you cannot install Python or <code>aria2c</code>, <code>wget</code> is usually available by default.</p>"},{"location":"data_management/downloading-large-datasets/#download-using-wget","title":"Download Using <code>wget</code>","text":"<pre><code>wget -c -i urls.txt\n</code></pre> <ul> <li><code>-c</code>: Resume partially downloaded files</li> <li><code>-i</code>: Read URLs from a file</li> </ul>"},{"location":"data_management/downloading-large-datasets/#6-handling-the-data-read-vs-extract","title":"6. Handling the Data: Read vs. Extract","text":"<p>After downloading, you will have several large <code>.tar</code> files.</p>"},{"location":"data_management/downloading-large-datasets/#option-a-recommended-do-not-extract","title":"Option A: Recommended (Do NOT Extract)","text":"<p>STOP: Do NOT untar everything</p> <ul> <li>Risk: 330GB of archives expands to &gt;9TB when extracted.</li> <li>Result: You may exceed quotas or crash the filesystem.</li> <li>Best practice: Stream data directly from <code>.tar</code> files.</li> </ul> <p>Many analysis scripts can read compressed archives directly, avoiding massive disk usage.</p> <p>Benefits:</p> <ul> <li>Minimal disk usage</li> <li>Faster I/O</li> <li>No millions of tiny files</li> </ul>"},{"location":"data_management/downloading-large-datasets/#option-b-if-you-must-extract-advanced","title":"Option B: If You MUST Extract (Advanced)","text":"<p>Only proceed if you have &gt;10TB free space and a strict requirement to extract files.</p> <p>Safe extraction script:</p> <pre><code>#!/bin/bash\nfor tarfile in *.tar; do\n    dirname=\"${tarfile%.tar}\"\n    mkdir -p \"$dirname\"\n    tar -xf \"$tarfile\" -C \"$dirname\"\ndone\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#7-expert-mode-getting-links-without-zenodo_get","title":"7. Expert Mode: Getting Links Without <code>zenodo_get</code>","text":"<p>If Python is unavailable, you can query the Zenodo API directly.</p>"},{"location":"data_management/downloading-large-datasets/#generate-urlstxt-via-curl","title":"Generate <code>urls.txt</code> via <code>curl</code>","text":"<pre><code>curl -s https://zenodo.org/api/records/17725827 \\\n| grep -oP 'https://zenodo.org/api/records/17725827/files/[^\"]+' \\\n&gt; urls.txt\n</code></pre> <p>You can then use <code>wget</code> or <code>aria2c</code> with this file.</p>"},{"location":"data_management/downloading-large-datasets/#knowledge-check","title":"\ud83e\udde0 Knowledge Check","text":""},{"location":"data_management/downloading-large-datasets/#challenge-1-generate-links-without-downloading","title":"Challenge 1: Generate Links Without Downloading","text":"Solution <pre><code>zenodo_get 17725827 -w links.txt\ngrep \"communities_infrastructures.tar\" links.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-2-download-only-one-small-file","title":"Challenge 2: Download Only One Small File","text":"Solution <pre><code># wget\nwget -c &lt;URL&gt;\n\n# aria2c\naria2c -x 10 -s 10 &lt;URL&gt;\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-3-safe-extraction-test","title":"Challenge 3: Safe Extraction Test","text":"Best practice <pre><code>mkdir test_extract\ntar -xf communities_infrastructures.tar -C test_extract\nls -l test_extract\n</code></pre> <p>If files appear inside <code>test_extract</code>, you are ready for the full dataset.</p>"},{"location":"emacs/","title":"Emacs","text":""},{"location":"emacs/#emacs","title":"Emacs","text":""},{"location":"emacs/#org-mode","title":"ORG-Mode","text":""},{"location":"emacs/e1/","title":"Emacs","text":""},{"location":"emacs/e1/#emacs","title":"Emacs","text":"<ul> <li>Coming soon.</li> </ul>"}]}