{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to My Digital Notebook","text":""},{"location":"#welcome-to-my-digital-notebook","title":"\ud83d\udc4b Welcome to My Digital Notebook","text":"<p>This site is a small personal notebook where I write down technical notes and short tutorials that I use in my own work.</p> <p>The main purpose is simple: to avoid re-learning the same things again months later.</p> <p>Some of these notes may also be useful to others working with Linux systems, data handling, or research workflows.</p> <p>This notebook grows slowly and irregularly, depending on real needs.</p>"},{"location":"data_management/downloading-large-datasets/","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":""},{"location":"data_management/downloading-large-datasets/#tutorial-how-to-download-massive-datasets-from-zenodo","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":"<p>Overview</p> <ul> <li>Goal: Download the 330GB OpenAIRE Graph dataset safely.</li> <li>Time Required: ~10 minutes to set up (download time depends on bandwidth).</li> <li>Skill Level: Beginner / Intermediate.</li> <li>Prerequisites: Access to a terminal (Linux/macOS) and ~350GB of free disk space.</li> <li>Tools Used: <code>zenodo_get</code>, <code>aria2c</code>, or <code>wget</code>.</li> </ul> <p>This guide explains how to reliably download massive datasets (100GB to terabytes) from Zenodo to a local server or a High-Performance Computing (HPC) cluster.</p> <p>As a practical example, we will be using the OpenAIRE Graph dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large text corpora).</p>"},{"location":"data_management/downloading-large-datasets/#1-context-what-are-we-downloading","title":"1. Context: What are we downloading?","text":""},{"location":"data_management/downloading-large-datasets/#what-is-zenodo","title":"What is Zenodo?","text":"<p>Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from any field of research and issues a persistent DOI for every record.</p>"},{"location":"data_management/downloading-large-datasets/#the-example-openaire-graph","title":"The Example: OpenAIRE Graph","text":"<p>In this tutorial, we are downloading the OpenAIRE Graph, one of the world\u2019s largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.</p> <ul> <li>Note on freshness: Massive datasets on Zenodo are usually static snapshots. For example, the OpenAIRE Graph dump is published roughly every six months. While live portals show real-time data, the Zenodo dump is the standard choice for stable, offline analysis.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#2-the-golden-rule-of-large-downloads","title":"2. The \u201cGolden Rule\u201d of Large Downloads","text":"<p>Do NOT use the \u201cDownload all\u201d button</p> <p>Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process will time out, does not support resuming, and provides no checksum verification. Always download files individually.</p> <p>On Zenodo record pages you may see a \u201cDownload all\u201d button pointing to a <code>files-archive</code> link.</p> <ul> <li>Why avoid it? Zenodo tries to create a single huge zip stream on the fly.</li> <li>Consequence: If the download fails near the end, you must restart from zero.</li> </ul> <p>Solution: Always download files individually using scripted tools.</p>"},{"location":"data_management/downloading-large-datasets/#3-method-1-the-easiest-way-zenodo_get","title":"3. Method 1: The Easiest Way (<code>zenodo_get</code>)","text":"<p><code>zenodo_get</code> is a community-maintained Python tool that handles file lists, retries, and checksum verification automatically.</p>"},{"location":"data_management/downloading-large-datasets/#step-a-installation","title":"Step A: Installation","text":"<pre><code>pip install zenodo-get\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-identify-the-record-id","title":"Step B: Identify the Record ID","text":"<p>You only need the record ID from the dataset URL.</p> <ul> <li>Example URL: <code>https://zenodo.org/records/17725827</code></li> <li>Record ID: <code>17725827</code></li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-c-robust-download-command","title":"Step C: Robust Download Command","text":"<pre><code>zenodo_get 17725827 -R 5 -p 2\n</code></pre> <p>Flag explanation:</p> <ul> <li><code>-R 5</code>: Retry failed downloads up to 5 times.</li> <li><code>-p 2</code>: Pause 2 seconds between retries.</li> </ul>"},{"location":"data_management/downloading-large-datasets/#resuming-a-failed-download","title":"Resuming a Failed Download","text":"<p>If the process stops, simply rerun the same command. Completed and checksum-verified files will be skipped automatically.</p>"},{"location":"data_management/downloading-large-datasets/#4-method-2-the-high-speed-way-aria2c","title":"4. Method 2: The High-Speed Way (<code>aria2c</code>)","text":"<p><code>aria2c</code> is ideal for fast but unstable connections. It supports parallel connections per file.</p>"},{"location":"data_management/downloading-large-datasets/#advantages","title":"Advantages","text":"<ul> <li>Parallel downloads per file</li> <li>Excellent resume support</li> <li>Efficient bandwidth utilization</li> </ul>"},{"location":"data_management/downloading-large-datasets/#step-a-generate-the-url-list","title":"Step A: Generate the URL List","text":"<pre><code>zenodo_get 17725827 -w urls.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#step-b-download-with-aria2c","title":"Step B: Download with <code>aria2c</code>","text":"<pre><code>aria2c -i urls.txt -x 10 -c\n</code></pre> <ul> <li><code>-x 10</code>: Up to 10 connections per file</li> <li><code>-c</code>: Resume interrupted downloads</li> </ul>"},{"location":"data_management/downloading-large-datasets/#5-method-3-the-standard-way-wget","title":"5. Method 3: The Standard Way (<code>wget</code>)","text":"<p>If you cannot install Python or <code>aria2c</code>, <code>wget</code> is usually available by default.</p>"},{"location":"data_management/downloading-large-datasets/#download-using-wget","title":"Download Using <code>wget</code>","text":"<pre><code>wget -c -i urls.txt\n</code></pre> <ul> <li><code>-c</code>: Resume partially downloaded files</li> <li><code>-i</code>: Read URLs from a file</li> </ul>"},{"location":"data_management/downloading-large-datasets/#6-handling-the-data-read-vs-extract","title":"6. Handling the Data: Read vs. Extract","text":"<p>After downloading, you will have several large <code>.tar</code> files.</p>"},{"location":"data_management/downloading-large-datasets/#option-a-recommended-do-not-extract","title":"Option A: Recommended (Do NOT Extract)","text":"<p>STOP: Do NOT untar everything</p> <ul> <li>Risk: 330GB of archives expands to &gt;9TB when extracted.</li> <li>Result: You may exceed quotas or crash the filesystem.</li> <li>Best practice: Stream data directly from <code>.tar</code> files.</li> </ul> <p>Many analysis scripts can read compressed archives directly, avoiding massive disk usage.</p> <p>Benefits:</p> <ul> <li>Minimal disk usage</li> <li>Faster I/O</li> <li>No millions of tiny files</li> </ul>"},{"location":"data_management/downloading-large-datasets/#option-b-if-you-must-extract-advanced","title":"Option B: If You MUST Extract (Advanced)","text":"<p>Only proceed if you have &gt;10TB free space and a strict requirement to extract files.</p> <p>Safe extraction script:</p> <pre><code>#!/bin/bash\nfor tarfile in *.tar; do\n    dirname=\"${tarfile%.tar}\"\n    mkdir -p \"$dirname\"\n    tar -xf \"$tarfile\" -C \"$dirname\"\ndone\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#7-expert-mode-getting-links-without-zenodo_get","title":"7. Expert Mode: Getting Links Without <code>zenodo_get</code>","text":"<p>If Python is unavailable, you can query the Zenodo API directly.</p>"},{"location":"data_management/downloading-large-datasets/#generate-urlstxt-via-curl","title":"Generate <code>urls.txt</code> via <code>curl</code>","text":"<pre><code>curl -s https://zenodo.org/api/records/17725827 \\\n| grep -oP 'https://zenodo.org/api/records/17725827/files/[^\"]+' \\\n&gt; urls.txt\n</code></pre> <p>You can then use <code>wget</code> or <code>aria2c</code> with this file.</p>"},{"location":"data_management/downloading-large-datasets/#knowledge-check","title":"\ud83e\udde0 Knowledge Check","text":""},{"location":"data_management/downloading-large-datasets/#challenge-1-generate-links-without-downloading","title":"Challenge 1: Generate Links Without Downloading","text":"Solution <pre><code>zenodo_get 17725827 -w links.txt\ngrep \"communities_infrastructures.tar\" links.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-2-download-only-one-small-file","title":"Challenge 2: Download Only One Small File","text":"Solution <pre><code># wget\nwget -c &lt;URL&gt;\n\n# aria2c\naria2c -x 10 -s 10 &lt;URL&gt;\n</code></pre>"},{"location":"data_management/downloading-large-datasets/#challenge-3-safe-extraction-test","title":"Challenge 3: Safe Extraction Test","text":"Best practice <pre><code>mkdir test_extract\ntar -xf communities_infrastructures.tar -C test_extract\nls -l test_extract\n</code></pre> <p>If files appear inside <code>test_extract</code>, you are ready for the full dataset.</p>"},{"location":"data_management/downloading-large-datasets_backup/","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":""},{"location":"data_management/downloading-large-datasets_backup/#tutorial-how-to-download-massive-datasets-from-zenodo","title":"Tutorial: How to Download Massive Datasets from Zenodo","text":"<p>Overview</p> <ul> <li>Goal: Download the 330GB OpenAIRE Graph dataset safely.</li> <li>Time Required: ~10 minutes to set up (Download time depends on bandwidth).</li> <li>Skill Level: Beginner / Intermediate.</li> <li>Prerequisites: Access to a terminal (Linux/Mac) and ~350GB of free disk space.</li> <li>Tools Used: <code>zenodo_get</code>, <code>aria2c</code>, or <code>wget</code>.</li> </ul> <p>This guide explains how to reliably download massive datasets (100GB to Terabytes) from Zenodo to a local server or High-Performance Computing (HPC) cluster.</p> <p>As a practical example, we will be using the OpenAIRE Graph dataset (~330GB), but these methods apply to any large Zenodo record (e.g., climate data, genomic sequences, or large corpus text dumps).</p>"},{"location":"data_management/downloading-large-datasets_backup/#1-context-what-are-we-downloading","title":"1. Context: What are we downloading?","text":""},{"location":"data_management/downloading-large-datasets_backup/#what-is-zenodo","title":"What is Zenodo?","text":"<p>Zenodo is an open-access repository developed under the European OpenAIRE program and operated by CERN. It hosts datasets, software, and reports from any field of research. It is widely used because it issues a persistent DOI (Digital Object Identifier) for every record.</p>"},{"location":"data_management/downloading-large-datasets_backup/#the-example-openaire-graph","title":"The Example: OpenAIRE Graph","text":"<p>In this tutorial, we are downloading the OpenAIRE Graph, one of the world's largest open scholarly knowledge graphs. It connects millions of publications, datasets, software, and funding records.</p> <ul> <li>Note on \"Freshness\": Massive datasets on Zenodo are often static snapshots. For example, the OpenAIRE Graph dump is published every 6 months. While live portals (like OpenAIRE EXPLORE) show real-time data, the Zenodo dump is the standard source for researchers needing a stable, offline version of the entire database.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#2-the-golden-rule-of-large-downloads","title":"2. The \"Golden Rule\" of Large Downloads","text":"<p>Stop: Do NOT use the 'Download All' button</p> <p>Zenodo attempts to zip the files on the fly. For a 330GB dataset, this process  will time out, fails to support resuming, and gives you no way to verify checksums. Always download the files individually.</p> <p>On any Zenodo record page, you will see a button labeled \"Download all\" that points to a <code>files-archive</code> link.</p> <ul> <li>Why avoid it? Zenodo attempts to zip hundreds of gigabytes on-the-fly into a single stream. This almost always fails due to network timeouts or browser limits.</li> <li>The Consequence: It cannot be resumed. If it fails at 99%, you lose everything.</li> </ul> <p>The Solution: Always download the files individually. The instructions below show you how to automate this.</p>"},{"location":"data_management/downloading-large-datasets_backup/#3-method-1-the-easiest-way-zenodo_get","title":"3. Method 1: The Easiest Way (<code>zenodo_get</code>)","text":"<p>The community has created a dedicated Python tool called <code>zenodo_get</code>. This is the recommended method for most users because it handles the file list, retries, and checksum verification automatically.</p>"},{"location":"data_management/downloading-large-datasets_backup/#step-a-installation","title":"Step A: Installation","text":"<p>You need Python installed. Run this in your terminal:</p> <pre><code>pip install zenodo-get\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#step-b-identify-the-record-id","title":"Step B: Identify the Record ID","text":"<p>You only need the Record ID from the URL of your dataset.</p> <ul> <li>Example URL: <code>https://zenodo.org/records/17725827</code></li> <li>Record ID: <code>17725827</code></li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#step-c-the-robust-download-command","title":"Step C: The Robust Download Command","text":"<p>Run the following command. We will add flags to handle network blips automatically.</p> <pre><code>zenodo_get 17725827 -R 5 -p 2\n</code></pre> <p>What do these flags do?</p> <ul> <li><code>17725827</code>: The ID of the dataset you want.</li> <li><code>-R 5</code>: Retry limit. If a file fails to download, the tool will try 5 more times before giving up.</li> <li><code>-p 2</code>: Pause. It will wait 2 seconds between retries to let the connection stabilize.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#how-to-resume-if-it-crashes","title":"How to Resume if it crashes?","text":"<p>If your internet drops completely and the script stops, simply run the exact same command again.</p> <ul> <li>The tool checks the folder.</li> <li>If a file exists and is complete (verified by MD5), it skips it.</li> <li>If a file is missing or corrupt, it downloads it again.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#4-method-2-the-high-speed-way-aria2c","title":"4. Method 2: The High-Speed Way (<code>aria2c</code>)","text":"<p>If you have a fast internet connection but it is unstable, or if you want to maximize speed, <code>aria2c</code> is often superior to standard tools because it supports parallel connections.</p>"},{"location":"data_management/downloading-large-datasets_backup/#advantages-of-aria2c","title":"Advantages of <code>aria2c</code>","text":"<ul> <li>Parallel Connections: It opens multiple connections per file (like a torrent), filling your bandwidth pipe more effectively than a single stream.</li> <li>Robustness: It handles dropouts extremely well.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#step-a-get-the-links-first","title":"Step A: Get the Links first","text":"<p>Use <code>zenodo_get</code> just to generate the list of URLs, without downloading the data yet.</p> <pre><code># -w creates a text file of URLs\nzenodo_get 17725827 -w urls.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#step-b-download-with-aria2c","title":"Step B: Download with <code>aria2c</code>","text":"<p>Feed the URL list into <code>aria2c</code>:</p> <pre><code>aria2c -i urls.txt -x 10 -c\n</code></pre> <ul> <li><code>-i urls.txt</code>: The input file containing the list of links.</li> <li><code>-x 10</code>: Max Connections. Uses up to 10 connections per file to speed up the download.</li> <li><code>-c</code>: Continue. This is critical. If interrupted, running this command again resumes exactly where it stopped.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#5-method-3-the-standard-way-wget","title":"5. Method 3: The Standard Way (<code>wget</code>)","text":"<p>If you are on a strict university system where you cannot install Python or <code>aria2</code>, <code>wget</code> is likely already installed.</p>"},{"location":"data_management/downloading-large-datasets_backup/#advantages-of-wget","title":"Advantages of <code>wget</code>","text":"<ul> <li>Universally Available: It is on almost every Linux server by default.</li> <li>Stable: It is single-threaded, which is friendlier to strict firewalls that might block the aggressive connection opening of <code>aria2c</code>.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#how-to-download","title":"How to Download","text":"<p>First, you need the list of links (see \"Expert Mode\" below if you can't use <code>zenodo_get</code>). Then run:</p> <pre><code>wget -c -i urls.txt\n</code></pre> <ul> <li><code>-c</code>: Continue. This is the \"resume\" flag. Without it, <code>wget</code> will restart files from zero if the connection breaks.</li> <li><code>-i</code>: Input file. Tells wget to read links from your text file.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#6-step-4-how-to-handle-the-data-read-vs-extract","title":"6. Step 4: How to Handle the Data (Read vs. Extract)","text":"<p>Once the download is complete, you will have several massive <code>.tar</code> files. Stop and read this before doing anything else.</p>"},{"location":"data_management/downloading-large-datasets_backup/#option-a-the-recommended-way-do-not-extract","title":"Option A: The Recommended Way (Do NOT Extract)","text":"<p>STOP: Do NOT untar these files!</p> <p>You do not need to extract these files.</p> <ul> <li>The Risk: The compressed dataset is ~330GB, but if you extract (untar) everything, it expands to over 9TB of JSON text. This will likely fill your storage quota immediately and crash the file system.</li> </ul> <ul> <li>The Solution: Most modern analysis tools can read directly from compressed archives.</li> </ul> <p>The example script provided in this repository (<code>count_tubitak_papers.py</code>) is designed to stream the data directly from the <code>.tar</code> files without ever extracting them to disk.</p> <p>Why this is better:</p> <ul> <li>Disk Space: You only use the 330GB for the archives.</li> <li>Efficiency: The script reads the stream into memory buffer-by-buffer. It never creates the massive intermediate files.</li> <li>Speed: Reading one large <code>.tar</code> file is faster for the file system than reading millions of tiny <code>.json</code> files.</li> </ul>"},{"location":"data_management/downloading-large-datasets_backup/#option-b-if-you-must-extract-advanced","title":"Option B: If you MUST Extract (Advanced)","text":"<p>Only follow this step if you have &gt;10TB of free storage and a specific software requirement that prevents reading from streams.</p> <p>If you proceed, do not simply run <code>tar -xvf *.tar</code>. The archives contain millions of small files that will \"explode\" into your current directory, making it impossible to list files or clean up.</p> <p>The Safe Extraction Method: Use this script to unpack each tarball into its own dedicated subdirectory.</p> <pre><code>#!/bin/bash\n# Loop through all .tar files in the current directory\nfor tarfile in *.tar; do\n    # 1. Get the filename without the extension (e.g., 'publication_5')\n    dirname=\"${tarfile%.tar}\"\n\n    # 2. Create a clean folder for this archive\n    echo \"Creating directory: $dirname\"\n    mkdir -p \"$dirname\"\n\n    # 3. Extract the contents INTO that folder\n    echo \"Extracting $tarfile into $dirname...\"\n    tar -xf \"$tarfile\" -C \"$dirname\"\ndone\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#7-expert-mode-getting-links-without-zenodo_get","title":"7. Expert Mode: Getting Links without <code>zenodo_get</code>","text":"<p>What if you are on a locked-down server with no Python/pip allowed, and you need to generate the <code>urls.txt</code> file manually? You can interact with the Zenodo API directly using <code>curl</code>.</p>"},{"location":"data_management/downloading-large-datasets_backup/#the-command","title":"The Command","text":"<p>This one-liner fetches the metadata for the record, parses the JSON, and extracts the direct download links into a file named <code>urls.txt</code>.</p> <pre><code>curl -s \"[https://zenodo.org/api/records/17725827](https://zenodo.org/api/records/17725827)\" | grep -oP '[https://zenodo.org/api/records/17725827/files/](https://zenodo.org/api/records/17725827/files/)[^\"]+' &gt; urls.txt\n</code></pre> <ul> <li>Explanation:</li> <li><code>curl -s</code>: Fetches the data silently.</li> <li><code>grep -oP</code>: Searches for the pattern of file URLs.</li> <li><code>&gt; urls.txt</code>: Saves the result to a file.</li> </ul> <p>You can then feed this <code>urls.txt</code> into <code>wget</code> or <code>aria2c</code> as shown above.</p> <pre><code>\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#knowledge-check-try-it-yourself","title":"\ud83e\udde0 Knowledge Check: Try it Yourself","text":"<p>Before you commit to downloading 330GB, let's practice the workflow on the smallest file in the dataset (<code>communities_infrastructures.tar</code> - 41 kB).</p>"},{"location":"data_management/downloading-large-datasets_backup/#challenge-1-get-the-specific-link","title":"Challenge 1: Get the specific link","text":"<p>How can you generate the list of URLs without starting the download, so you can find the link for the small file?</p> Click to see the solution <p>Use the <code>-w</code> (write URLs) flag with <code>zenodo_get</code>:</p> <pre><code># 1. Generate the list\nzenodo_get 17725827 -w links.txt\n\n# 2. Search for the specific file inside the list\ngrep \"communities_infrastructures.tar\" links.txt\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#challenge-2-download-only-the-small-file","title":"Challenge 2: Download only the small file","text":"<p>Now that you have the link, how do you download just this file using <code>wget</code> or <code>aria2c</code>?</p> Click to see the solution <pre><code># Replace &lt;URL&gt; with the link you found in the previous step\nwget -c &lt;URL&gt;\n\n# Replace &lt;URL&gt; with the link you found in the previous step\naria2c -x 10 -s 10 &lt;URL&gt;\n</code></pre>"},{"location":"data_management/downloading-large-datasets_backup/#challenge-3-the-safe-extraction-test","title":"Challenge 3: The \"Safe Extraction\" Test","text":"<p>You now have <code>communities_infrastructures.tar</code>. How do you extract it safely into its own folder?</p> Click to see the best practice solution <p>Do not run <code>tar -xvf</code> in the root. Instead:</p> <p></p><pre><code># 1. Create the safe folder\nmkdir test_extract\n\n# 2. Extract into that folder\ntar -xf communities_infrastructures.tar -C test_extract\n\n# 3. Verify\nls -l test_extract/\n</code></pre> If you see a list of <code>.json</code> or <code>.gz</code> files inside, you are ready for the full dataset!<p></p>"},{"location":"emacs/","title":"Emacs","text":""},{"location":"emacs/#emacs","title":"Emacs","text":""},{"location":"emacs/#org-mode","title":"ORG-Mode","text":""},{"location":"emacs/e1/","title":"Emacs","text":""},{"location":"emacs/e1/#emacs","title":"Emacs","text":"<ul> <li>Coming soon.</li> </ul>"}]}